package glr

import (
	"errors"

	// "io"

	// "text/scanner"
	// "unicode"
	"go/scanner"
	"go/token"
)

type nodeType int

const (
	op nodeType = iota + 1
	label
)

// lexer implements yyLexer interface for the parser generated by goyacc
type lexer struct {
	s        scanner.Scanner
	err      error
	TokenVal string
}

func newLexer(src string) *lexer {
	// src is the input that we want to tokenize.
	// fmt.Printf("src: %q\n", src)
	srcbs := []byte(src)
	// fmt.Printf("srcbs: %#v\n", srcbs)

	// Initialize the scanner.
	var s scanner.Scanner
	fset := token.NewFileSet()                         // positions are relative to fset
	file := fset.AddFile("", fset.Base(), len(srcbs))  // register input "file"
	s.Init(file, srcbs, nil /* no error handler */, 0) //, scanner.ScanComments)

	return &lexer{
		s: s,
	}
}

func (l *lexer) Error(msg string) {
	l.err = errors.New(msg)
}

// yySymType is generated by goyacc
func (l *lexer) Lex(lval *yySymType) int {
	for {
		_, tok, lit := l.s.Scan()

		// Skip whitespace and semicolons
		if tok == token.SEMICOLON {
			continue
		}
		if tok == token.EOF {
			l.TokenVal = "$end"
			return -1
		}

		// Handle identifiers and special tokens
		if tok == token.IDENT {
			l.TokenVal = lit
			lval.token = lit
			switch lit {
			case "A":
				return A
			case "B":
				return B
			case "C":
				return C
			case "D":
				return D
			case "X":
				return X
			case "Y":
				return Y
			}
		}

		// For any other token, treat as illegal but keep parsing
		l.TokenVal = lit
		lval.token = lit
		return ILLEGAL
	}
}
